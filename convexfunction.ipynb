{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_spd_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data_unfixed(m=100, n=2, noise=0.01):\n",
    "    \"\"\"\n",
    "    Generates synthetic training data:\n",
    "    - x_i ~ N(2,1)\n",
    "    - y_i = Ax_i + b + noise\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    X = np.random.normal(loc=2, scale=1, size=(m, n))\n",
    "    A = np.random.normal(0, 1, size=(n,)) \n",
    "    b = np.random.normal(0, 1)\n",
    "    eta_i = np.random.normal(0, noise, size=(m,))\n",
    "    y = X @ A + b + eta_i\n",
    "    true_coefficients = {'A': A, 'b': b}\n",
    "    return X, y, true_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data_fixed(m=100, n=2, noise=0.01, model_type='linear', nonlinear_func=None):\n",
    "    \"\"\"\n",
    "    Generates synthetic training data with fixed A and b coefficients for linear regression, polynomial regression, and nonlinear cases.\n",
    "    \"\"\"\n",
    "    X = np.random.normal(loc=2, scale=1, size=(m, n))\n",
    "    if model_type == 'linear':\n",
    "        A = np.array([1.0, 2.0])[:n]  \n",
    "        b = 1.0                       \n",
    "        eta_i = np.random.normal(0, noise, size=(m,))\n",
    "        y = X @ A + b + eta_i\n",
    "        true_coefficients = {'A': A, 'b': b}\n",
    "        \n",
    "    elif model_type == 'polynomial':\n",
    "        A_poly = np.array([1.0, 0.5, 0.3, 0.2])\n",
    "        b = 1.0\n",
    "        eta_i = np.random.normal(0, noise, size=(m,))\n",
    "        X_poly = np.hstack([X, X**2])  \n",
    "        y = X_poly @ A_poly + b + eta_i\n",
    "        true_coefficients = {'A': A_poly, 'b': b}\n",
    "        X = X_poly\n",
    "    \n",
    "    elif model_type == 'nonlinear':\n",
    "        b = 1.0\n",
    "        eta_i = np.random.normal(0, noise, size=(m,))\n",
    "        y = nonlinear_func(X[:, 0]) + b + eta_i\n",
    "        true_coefficients = {'function': nonlinear_func.__name__, 'b': b}\n",
    "    \n",
    "    return X, y, true_coefficients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need model to fit true data, variable issue, have model have fit true data\n",
    "# y_i = Ax_i + b + noise\n",
    "# linear\n",
    "# polynomial\n",
    "# tradeoff: higher-deg is richer, but more parameters therefore need more data\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\"\"\n",
    "    Stochastic Gradient Descent for Regression\n",
    "    The model is trained on generated synthetic data to minimize mean \n",
    "        Model (Linear Case):\n",
    "            - Prediction: ŷ_i = w_0 + w_1 x_i1 + ... + w_n x_in\n",
    "            - Loss: f_i(w) = (1/2) (w^T x_{i,new} - y_i)^2\n",
    "            - Objective: F(w) = (1/m) Σ f_i(w)\n",
    "            - Gradient: ∇f_i(w) = (w^T x_{i,new} - y_i) x_{i,new}\n",
    "        \n",
    "        Model (Polynomial Case):\n",
    "            - Prediction: ŷ_i = w_0 + w_1 x_i1 + w_2 x_i1^2\n",
    "    \"\"\"\"\"\n",
    "    def __init__(self, X, y, num_iterations=1000):\n",
    "        \"\"\"\n",
    "        Initializes the SGD model with generated data.\n",
    "\n",
    "        Args:\n",
    "            X: The input data matrix of shape (m, n), where m is the number of samples \n",
    "                and n is the number of features.\n",
    "            y: The output data vector of shape (m,).\n",
    "            num_iterations: The number of iterations for running the SGD optimization. Defaults to 1000.\n",
    "        \"\"\"\n",
    "        self.X = X \n",
    "        self.y = y \n",
    "        self.m, self.n = X.shape  \n",
    "        \n",
    "        self.X_new = np.hstack([np.ones((self.m, 1)), self.X])  \n",
    "        self.n_new = self.n + 1  \n",
    "        \n",
    "        self.num_iterations = num_iterations\n",
    "        self.w = np.zeros(self.n_new)  \n",
    "        \n",
    "        self.w_star = np.linalg.solve(self.X_new.T @ self.X_new, self.X_new.T @ self.y.flatten())\n",
    "        self.F_star = self.F(self.w_star)\n",
    "        print(self.w_star)\n",
    "        \n",
    "    def f_i(self, w, i):\n",
    "        \"\"\"\n",
    "        Computes the loss for a single sample.\n",
    "\n",
    "        Args:\n",
    "            w: The parameter vector.\n",
    "            i: The index of the current training sample.\n",
    "\n",
    "        Returns:\n",
    "            he loss for the i-th training sample.\n",
    "        \"\"\"\n",
    "        return 0.5 * (self.X_new[i] @ w - self.y[i]) ** 2\n",
    "    \n",
    "    def grad_f_i(self, w, i):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the loss function with respect to parameters for a single sample.\n",
    "\n",
    "        Args:\n",
    "            w: The parameter vector.\n",
    "            i: The index of the current training sample.\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss function for the i-th sample.\n",
    "        \"\"\"\n",
    "        return (self.X_new[i] @ w - self.y[i]) * self.X_new[i]\n",
    "    \n",
    "\n",
    "    def F(self, w):\n",
    "        \"\"\"\n",
    "        Computes the average loss over all samples.\n",
    "\n",
    "        Args:\n",
    "            w: The parameter vector.\n",
    "\n",
    "        Returns:\n",
    "            float: The average loss over all samples.\n",
    "        \"\"\"\n",
    "        F = (1/self.m) * sum(self.f_i(w, j) for j in range(self.m))\n",
    "        return F\n",
    "\n",
    "    def grad_F(self, w):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the objective function with respect to the parameters.\n",
    "\n",
    "        Args:\n",
    "            w: The parameter vector.\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the objective function.\n",
    "        \"\"\"\n",
    "        grad_F = (1/self.m) * sum(self.grad_f_i(w, i) for i in range(self.m))\n",
    "        return grad_F\n",
    "    \n",
    "    def stochastic_grad(self):\n",
    "        \"\"\"\n",
    "        Computes the stochastic gradient (using a random training sample).\n",
    "\n",
    "        Returns:\n",
    "            The stochastic gradient based on a randomly selected training sample.\n",
    "        \"\"\"\n",
    "        i = np.random.randint(0, self.m)\n",
    "        grad = self.grad_f_i(self.w, i)\n",
    "        return grad \n",
    "    \n",
    "    def mini_batch_grad(self):\n",
    "        \"\"\"\n",
    "        Computes the mini-batch gradient (using a random subset of training samples).\n",
    "\n",
    "        Args:\n",
    "            batch_size: The number of samples in the mini-batch.\n",
    "\n",
    "        Returns:\n",
    "            The mini-batch gradient.\n",
    "        \"\"\"\n",
    "        indices = np.random.choice(self.n, self.batch_size, replace=False)\n",
    "        return (1 / self.batch_size) * sum(self.grad_f_i(self.w, i) for i in indices)\n",
    "    \n",
    "    def compute_L(self, num_samples=1000):\n",
    "        \"\"\"\n",
    "        Computes the Lipschitz constant L of the gradient of the objective function.\n",
    "\n",
    "        Args:\n",
    "            num_samples: The number of random samples to estimate L. Default is 1000.\n",
    "\n",
    "        Returns:\n",
    "            The estimated Lipschitz constant.\n",
    "        \"\"\"\n",
    "        L_vals = []\n",
    "        d = self.X_new.shape[1]\n",
    "        for _ in range(num_samples):\n",
    "            w1, w2 = np.random.randn(d), np.random.randn(d)\n",
    "            grad_diff = np.linalg.norm(self.grad_F(w1) - self.grad_F(w2), 2)\n",
    "            w_diff = np.linalg.norm(w1 - w2, 2)\n",
    "            \n",
    "            if w_diff > 1e-8: \n",
    "                L_vals.append(grad_diff / w_diff)\n",
    "        return max(L_vals) if L_vals else 1.0\n",
    "    \n",
    "    def compute_c(self):\n",
    "        \"\"\"\n",
    "        Computes the constant c associated with strong convexity (Assumption 4.5).\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): The number of random samples to estimate L. Default is 1000.\n",
    "\n",
    "        Returns:\n",
    "            float: The constant c.\n",
    "        \"\"\"\n",
    "        H = (1/self.m) * (self.X_new.T @ self.X_new)\n",
    "        eigenvalues = np.linalg.eigvalsh(H)\n",
    "        c = min(eigenvalues)\n",
    "        return c\n",
    "    \n",
    "    def estimate_parameters(self):\n",
    "        \"\"\"\n",
    "        Estimates the parameters bounding the variance of the gradient updates.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Estimated parameters (mu, mu_G, M, M_V, M_G).\n",
    "        \"\"\"\n",
    "        mu = 1 \n",
    "        mu_G = 1\n",
    "        M = 3\n",
    "        M_V = 1.5\n",
    "        M_G = M_V + mu_G ** 2\n",
    "        return mu, mu_G, M, M_V, M_G\n",
    "            \n",
    "        \n",
    "    def compute_fixed_stepsize(self):\n",
    "        \"\"\"\n",
    "        Computes the fixed stepsize for SGD using estimated parameters.\n",
    "\n",
    "        Returns:\n",
    "            float: The computed fixed stepsize for the SGD algorithm.\n",
    "        \"\"\"\n",
    "        L = self.compute_L()\n",
    "        c = self.compute_c()\n",
    "        M, mu, mu_G, M_V, M_G = self.estimate_parameters()\n",
    "        M_G = M_V + mu_G ** 2  \n",
    "        alpha = mu / (L * M_G)\n",
    "        print(f\"Parameters: L = {L}, c = {c}, M_V = {M_V}, mu = {mu}, mu_G = {mu_G}, M_G = {M_G} \\n Fixed Stepsize: alpha_bar = {alpha}\")\n",
    "        return alpha\n",
    "    \n",
    "#    def compute_diminishing_stepsize_params(self):\n",
    "    def optimize(self, stepsize_type='fixed'):\n",
    "        \"\"\"\n",
    "        Runs the SGD optimization process for a specified number of iterations.\n",
    "\n",
    "        Args:\n",
    "            stepsize_type: 'fixed' or 'diminishing' stepsize.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Optimized parameters, the history of the objective function, gradient norms, and \n",
    "                   distance to the optimal solution.\n",
    "        \"\"\"\n",
    "        alpha = self.compute_fixed_stepsize()\n",
    "    \n",
    "        w = self.w.copy()\n",
    "        obj_history = [self.F(w)]\n",
    "        grad_norm_history = [np.linalg.norm(self.grad_F(w))**2]\n",
    "        dist_to_opt_history = [np.linalg.norm(w - self.w_star)**2]\n",
    "    \n",
    "        for k in range(self.num_iterations):\n",
    "            alpha_k = alpha\n",
    "        \n",
    "            self.w = w\n",
    "            g_k = self.stochastic_grad()\n",
    "        \n",
    "            w -= alpha_k * g_k\n",
    "        \n",
    "            obj_history.append(self.F(w))\n",
    "            grad_norm_history.append(np.linalg.norm(self.grad_F(w))**2)\n",
    "            dist_to_opt_history.append(np.linalg.norm(w - self.w_star)**2)\n",
    "    \n",
    "        return w, np.array(obj_history), np.array(grad_norm_history), np.array(dist_to_opt_history)\n",
    "\n",
    "# mess around with step size\n",
    "# mess around with (x,y) from different distribution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random training data\n",
    "# generate training data\n",
    "# x_i has norm(2,0,1)\n",
    "# for i=1,...,n\n",
    "    # y = Ax_I + b + ita_i \n",
    "# generating (x_i,y_i) to be used for SGD\n",
    "# different ways of generating synthetic data to make it robust\n",
    "#X, y, true_params = generate_training_data_fixed(m=200, n=2, noise=0.01)\n",
    "#sgd = SGD(X, y, num_iterations=1000)\n",
    "#w_fixed, obj_fixed, grad_fixed, dist_fixed = sgd.optimize(stepsize_type='fixed')\n",
    "#print(\"True parameters:\")\n",
    "#print(f\"A: {true_params['A']}, b: {true_params['b']}\")\n",
    "#print(f\"Learned parameters (fixed step size): w_0 (bias) = {w_fixed[0]}, w_1 = {w_fixed[1]}, w_2 = {w_fixed[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments_with_fixed_parameters():\n",
    "    #stepsizes = [0.001, 0.01, 0.1]\n",
    "    num_steps = [100, 1000, 5000]\n",
    "    noise_levels = [0.01, 0.1, 1.0]\n",
    "    \n",
    "    \n",
    "    for j, n_steps in enumerate(num_steps):\n",
    "        for k, noise in enumerate(noise_levels):\n",
    "            X, y, true_params = generate_training_data_fixed(100, 2, noise, 'linear')\n",
    "            sgd = SGD(X, y, num_iterations=n_steps)\n",
    "            w, obj, grad, dist = sgd.optimize()\n",
    "            label = f\"Steps={n_steps}, Noise={noise}\"\n",
    "            print(\"Linear function:\")\n",
    "            print(f\"{label}, Final Loss: {obj[-1]:.4f}\")\n",
    "            \n",
    "    for j, n_steps in enumerate(num_steps):\n",
    "        for k, noise in enumerate(noise_levels):\n",
    "            X_poly, y_poly, true_params_poly = generate_training_data_fixed(100, 2, noise, 'polynomial')\n",
    "            sgd_poly = SGD(X_poly, y_poly, num_iterations=n_steps)\n",
    "            w_poly, obj_poly, grad_poly, dist_poly = sgd_poly.optimize()\n",
    "            label = f\"Steps={n_steps}, Noise={noise}\"\n",
    "            print(\"Polynomial function:\")\n",
    "            print(f\"{label}, Final Loss: {obj_poly[-1]:.4f}\")\n",
    "    \n",
    "    for j, n_steps in enumerate(num_steps):\n",
    "        for k, noise in enumerate(noise_levels):\n",
    "            X_nonlin, y_nonlin, true_params_nonlin = generate_training_data_fixed(\n",
    "            100, 1, noise, 'nonlinear', nonlinear_func=np.cos\n",
    "        )\n",
    "            sgd_nonlin = SGD(X_nonlin, y_nonlin, num_iterations=n_step s)\n",
    "            w_nonlin, obj_nonlin, grad_nonlin, dist_nonlin = sgd_nonlin.optimize()\n",
    "            label = f\"Steps={n_steps}, Noise={noise}\"\n",
    "            print(\"Nonlinear function\")\n",
    "            print(f\"{label}, Final Loss: {obj_nonlin[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00131685 1.00052578 1.99852314]\n",
      "Parameters: L = 9.602295785336711, c = 0.08559861624666057, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.009918263024508118\n",
      "Linear function:\n",
      "Steps=100, Noise=0.01, Final Loss: 0.0165\n",
      "[0.9662271  1.01081898 2.00643226]\n",
      "Parameters: L = 10.49291146433752, c = 0.10236555925732833, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.009076422264857848\n",
      "Linear function:\n",
      "Steps=100, Noise=0.1, Final Loss: 0.0601\n",
      "[1.07920236 1.04551454 1.93085495]\n",
      "Parameters: L = 9.348227037501406, c = 0.10057344257421398, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.010187824370978314\n",
      "Linear function:\n",
      "Steps=100, Noise=1.0, Final Loss: 0.5094\n",
      "[0.99842154 1.0000324  2.00165686]\n",
      "Parameters: L = 10.176785504208413, c = 0.10972607904520881, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.009358367158147468\n",
      "Linear function:\n",
      "Steps=1000, Noise=0.01, Final Loss: 0.0009\n",
      "[1.02500772 0.99096921 1.99763361]\n",
      "Parameters: L = 10.326767889796335, c = 0.10808473302574821, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.00922244948801435\n",
      "Linear function:\n",
      "Steps=1000, Noise=0.1, Final Loss: 0.0092\n",
      "[1.72002941 0.81980166 1.89582517]\n",
      "Parameters: L = 10.724652460433665, c = 0.08917678864938793, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.008880296642661012\n",
      "Linear function:\n",
      "Steps=1000, Noise=1.0, Final Loss: 0.5214\n",
      "[0.9992289  1.00062224 2.00014901]\n",
      "Parameters: L = 9.857664575834438, c = 0.1101364664001052, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.009661324394376997\n",
      "Linear function:\n",
      "Steps=5000, Noise=0.01, Final Loss: 0.0000\n",
      "[1.04761139 0.9934478  1.98594768]\n",
      "Parameters: L = 9.925267831985229, c = 0.09513427052222252, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.00959551891699893\n",
      "Linear function:\n",
      "Steps=5000, Noise=0.1, Final Loss: 0.0046\n",
      "[0.7506331  1.17811895 1.90258323]\n",
      "Parameters: L = 9.553972656760587, c = 0.11981598804109965, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.009968428700777451\n",
      "Linear function:\n",
      "Steps=5000, Noise=1.0, Final Loss: 0.5237\n",
      "[0.99850525 1.00228846 0.49783391 0.29994071 0.20062558]\n",
      "Parameters: L = 88.36830428543539, c = 0.029612118561661276, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0010777404410802088\n",
      "Polynomial function:\n",
      "Steps=100, Noise=0.01, Final Loss: 0.4993\n",
      "[0.93631827 1.06663564 0.49917492 0.28596476 0.20153229]\n",
      "Parameters: L = 76.49928473113107, c = 0.035518416623531286, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0012449540616337096\n",
      "Polynomial function:\n",
      "Steps=100, Noise=0.1, Final Loss: 0.3759\n",
      "[ 1.49393812  1.10676941 -0.18715176  0.28839193  0.38514865]\n",
      "Parameters: L = 81.42350448032995, c = 0.03565203264450773, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0011696634263771166\n",
      "Polynomial function:\n",
      "Steps=100, Noise=1.0, Final Loss: 0.8189\n",
      "[1.00351543 1.00281676 0.49440259 0.29928782 0.20129388]\n",
      "Parameters: L = 77.08915078815774, c = 0.03591039952748123, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.001235427998160352\n",
      "Polynomial function:\n",
      "Steps=1000, Noise=0.01, Final Loss: 0.1106\n",
      "[1.02962846 1.02042483 0.46755235 0.29803553 0.20314246]\n",
      "Parameters: L = 64.7921813974609, c = 0.033757117206938476, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0014699010464529207\n",
      "Polynomial function:\n",
      "Steps=1000, Noise=0.1, Final Loss: 0.1132\n",
      "[1.45216087 0.67705757 0.57338309 0.369826   0.13957545]\n",
      "Parameters: L = 96.47037006208016, c = 0.03527432921361881, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0009872263906192966\n",
      "Polynomial function:\n",
      "Steps=1000, Noise=1.0, Final Loss: 0.7326\n",
      "[1.00290059 1.00146789 0.49517713 0.29993793 0.20083749]\n",
      "Parameters: L = 78.55980657242404, c = 0.026031353938253492, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0012123005311920612\n",
      "Polynomial function:\n",
      "Steps=5000, Noise=0.01, Final Loss: 0.0023\n",
      "[0.98916041 1.04141595 0.47900326 0.29068234 0.20171434]\n",
      "Parameters: L = 88.8574882540459, c = 0.029119560136284108, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0010718071949750258\n",
      "Polynomial function:\n",
      "Steps=5000, Noise=0.1, Final Loss: 0.0097\n",
      "[0.95972919 1.01152524 0.25821425 0.30054836 0.2913472 ]\n",
      "Parameters: L = 73.12043028317173, c = 0.04248825657659839, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.001302482696959919\n",
      "Polynomial function:\n",
      "Steps=5000, Noise=1.0, Final Loss: 0.4960\n",
      "[ 1.97188583 -0.61642548]\n",
      "Parameters: L = 5.734545411876826, c = 0.15286948941052714, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0166077846451172\n",
      "Nonlinear function\n",
      "Steps=100, Noise=0.01, Final Loss: 0.2113\n",
      "[ 1.80919639 -0.51535275]\n",
      "Parameters: L = 6.5052927814811214, c = 0.1548251419670098, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.014640093603352234\n",
      "Nonlinear function\n",
      "Steps=100, Noise=0.1, Final Loss: 0.2324\n",
      "[ 2.06293905 -0.62663715]\n",
      "Parameters: L = 5.602034803234248, c = 0.171803800956967, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.017000625412593118\n",
      "Nonlinear function\n",
      "Steps=100, Noise=1.0, Final Loss: 0.7297\n",
      "[ 1.75767008 -0.5064739 ]\n",
      "Parameters: L = 5.556590071134577, c = 0.23690767238000698, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.01713966551767764\n",
      "Nonlinear function\n",
      "Steps=1000, Noise=0.01, Final Loss: 0.0587\n",
      "[ 1.87825902 -0.55914811]\n",
      "Parameters: L = 5.914909265088711, c = 0.1605017982061856, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.016101361993871074\n",
      "Nonlinear function\n",
      "Steps=1000, Noise=0.1, Final Loss: 0.0430\n",
      "[ 1.94915141 -0.53829125]\n",
      "Parameters: L = 6.027364444383981, c = 0.17502552657506665, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.015800951828428708\n",
      "Nonlinear function\n",
      "Steps=1000, Noise=1.0, Final Loss: 0.5759\n",
      "[ 1.68500797 -0.45967761]\n",
      "Parameters: L = 6.429967013452265, c = 0.15790882417290997, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.014811599350174842\n",
      "Nonlinear function\n",
      "Steps=5000, Noise=0.01, Final Loss: 0.0723\n",
      "[ 1.90362445 -0.57802775]\n",
      "Parameters: L = 5.613606719475935, c = 0.17607618363334865, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.01696558024053853\n",
      "Nonlinear function\n",
      "Steps=5000, Noise=0.1, Final Loss: 0.0389\n",
      "[ 1.78900748 -0.54061771]\n",
      "Parameters: L = 6.085795526329993, c = 0.1461426877883385, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.015649243361209025\n",
      "Nonlinear function\n",
      "Steps=5000, Noise=1.0, Final Loss: 0.5271\n"
     ]
    }
   ],
   "source": [
    "run_experiments_with_fixed_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(15, 9))\n",
    "#plt.subplot(3, 1, 1)\n",
    "#plt.plot(obj_fixed - sgd.F_star, label='Fixed Stepsize')\n",
    "#plt.ylabel('$F(w) - F(w_*)$')\n",
    "#plt.title('Objective Function Decrease')\n",
    "#plt.legend()\n",
    "#plt.yscale('log')\n",
    "\n",
    "#plt.subplot(3, 1, 2)\n",
    "#plt.plot(grad_fixed, label='Fixed Stepsize')\n",
    "#plt.ylabel('$||F(w)||^2$')\n",
    "#plt.title('Gradient Norm Squared')\n",
    "#plt.legend()\n",
    "#plt.yscale('log')\n",
    "\n",
    "#plt.subplot(3, 1, 3)\n",
    "#plt.plot(dist_fixed, label='Fixed Stepsize')\n",
    "#plt.xlabel('Iteration')\n",
    "#plt.ylabel('$||w_k - w_*||^2$')\n",
    "#plt.title('Distance to Optimum')\n",
    "#plt.legend()\n",
    "#plt.yscale('log')\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
