{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_spd_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data_unfixed(m=100, n=2, noise=0.01):\n",
    "    \"\"\"\n",
    "    Generates synthetic training data:\n",
    "    - x_i ~ N(2,1)\n",
    "    - y_i = Ax_i + b + noise\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    X = np.random.normal(loc=2, scale=1, size=(m, n))\n",
    "    A = np.random.normal(0, 1, size=(n,)) \n",
    "    b = np.random.normal(0, 1)\n",
    "    eta_i = np.random.normal(0, noise, size=(m,))\n",
    "    y = X @ A + b + eta_i\n",
    "    true_coefficients = {'A': A, 'b': b}\n",
    "    return X, y, true_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data_fixed(m=100, n=2, noise=0.01, model_type='linear', nonlinear_func=None):\n",
    "    \"\"\"\n",
    "    Generates synthetic training data with fixed A and b coefficients for linear regression, polynomial regression, and nonlinear cases.\n",
    "    \"\"\"\n",
    "    X = np.random.normal(loc=2, scale=1, size=(m, n))\n",
    "    if model_type == 'linear':\n",
    "        A = np.array([1.0, 2.0])[:n]  \n",
    "        b = 1.0                       \n",
    "        eta_i = np.random.normal(0, noise, size=(m,))\n",
    "        y = X @ A + b + eta_i\n",
    "        true_coefficients = {'A': A, 'b': b}\n",
    "        print(true_coefficients)\n",
    "        \n",
    "    elif model_type == 'polynomial':\n",
    "        A_poly = np.array([1.0, 0.5, 0.3, 0.2])\n",
    "        b = 1.0\n",
    "        eta_i = np.random.normal(0, noise, size=(m,))\n",
    "        X_poly = np.hstack([X, X**2])  \n",
    "        y = X_poly @ A_poly + b + eta_i\n",
    "        true_coefficients = {'A': A_poly, 'b': b}\n",
    "        X = X_poly\n",
    "    \n",
    "    elif model_type == 'nonlinear':\n",
    "        b = 1.0\n",
    "        eta_i = np.random.normal(0, noise, size=(m,))\n",
    "        y = nonlinear_func(X[:, 0]) + b + eta_i\n",
    "        true_coefficients = {'function': nonlinear_func.__name__, 'b': b}\n",
    "    \n",
    "    return X, y, true_coefficients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need model to fit true data, variable issue, have model have fit true data\n",
    "# y_i = Ax_i + b + noise\n",
    "# linear\n",
    "# polynomial\n",
    "# tradeoff: higher-deg is richer, but more parameters therefore need more data\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\"\"\n",
    "    Stochastic Gradient Descent for Regression\n",
    "    The model is trained on generated synthetic data to minimize mean \n",
    "        Model (Linear Case):\n",
    "            - Prediction: ŷ_i = w_0 + w_1 x_i1 + ... + w_n x_in\n",
    "            - Loss: f_i(w) = (1/2) (w^T x_{i,new} - y_i)^2\n",
    "            - Objective: F(w) = (1/m) Σ f_i(w)\n",
    "            - Gradient: ∇f_i(w) = (w^T x_{i,new} - y_i) x_{i,new}\n",
    "        \n",
    "        Model (Polynomial Case):\n",
    "            - Prediction: ŷ_i = w_0 + w_1 x_i1 + w_2 x_i1^2\n",
    "    \"\"\"\"\"\n",
    "    def __init__(self, X, y, num_iterations=1000):\n",
    "        \"\"\"\n",
    "        Initializes the SGD model with generated data.\n",
    "\n",
    "        Args:\n",
    "            X: The input data matrix of shape (m, n), where m is the number of samples \n",
    "                and n is the number of features.\n",
    "            y: The output data vector of shape (m,).\n",
    "            num_iterations: The number of iterations for running the SGD optimization. Defaults to 1000.\n",
    "        \"\"\"\n",
    "        self.X = X \n",
    "        self.y = y \n",
    "        self.m, self.n = X.shape  \n",
    "        \n",
    "        self.X_new = np.hstack([np.ones((self.m, 1)), self.X])  \n",
    "        self.n_new = self.n + 1  \n",
    "        \n",
    "        self.num_iterations = num_iterations\n",
    "        self.w = np.zeros(self.n_new)  \n",
    "        \n",
    "        self.w_star = np.linalg.solve(self.X_new.T @ self.X_new, self.X_new.T @ self.y.flatten())\n",
    "        self.F_star = self.F(self.w_star)\n",
    "        print(self.w_star)\n",
    "        \n",
    "    def f_i(self, w, i):\n",
    "        \"\"\"\n",
    "        Computes the loss for a single sample.\n",
    "\n",
    "        Args:\n",
    "            w: The parameter vector.\n",
    "            i: The index of the current training sample.\n",
    "\n",
    "        Returns:\n",
    "            he loss for the i-th training sample.\n",
    "        \"\"\"\n",
    "        return 0.5 * (self.X_new[i] @ w - self.y[i]) ** 2\n",
    "    \n",
    "    def grad_f_i(self, w, i):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the loss function with respect to parameters for a single sample.\n",
    "\n",
    "        Args:\n",
    "            w: The parameter vector.\n",
    "            i: The index of the current training sample.\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the loss function for the i-th sample.\n",
    "        \"\"\"\n",
    "        return (self.X_new[i] @ w - self.y[i]) * self.X_new[i]\n",
    "    \n",
    "\n",
    "    def F(self, w):\n",
    "        \"\"\"\n",
    "        Computes the average loss over all samples.\n",
    "\n",
    "        Args:\n",
    "            w: The parameter vector.\n",
    "\n",
    "        Returns:\n",
    "            float: The average loss over all samples.\n",
    "        \"\"\"\n",
    "        F = (1/self.m) * sum(self.f_i(w, j) for j in range(self.m))\n",
    "        return F\n",
    "\n",
    "    def grad_F(self, w):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the objective function with respect to the parameters.\n",
    "\n",
    "        Args:\n",
    "            w: The parameter vector.\n",
    "\n",
    "        Returns:\n",
    "            The gradient of the objective function.\n",
    "        \"\"\"\n",
    "        grad_F = (1/self.m) * sum(self.grad_f_i(w, i) for i in range(self.m))\n",
    "        return grad_F\n",
    "    \n",
    "    def stochastic_grad(self):\n",
    "        \"\"\"\n",
    "        Computes the stochastic gradient (using a random training sample).\n",
    "\n",
    "        Returns:\n",
    "            The stochastic gradient based on a randomly selected training sample.\n",
    "        \"\"\"\n",
    "        i = np.random.randint(0, self.m)\n",
    "        grad = self.grad_f_i(self.w, i)\n",
    "        return grad \n",
    "    \n",
    "    def mini_batch_grad(self):\n",
    "        \"\"\"\n",
    "        Computes the mini-batch gradient (using a random subset of training samples).\n",
    "\n",
    "        Args:\n",
    "            batch_size: The number of samples in the mini-batch.\n",
    "\n",
    "        Returns:\n",
    "            The mini-batch gradient.\n",
    "        \"\"\"\n",
    "        indices = np.random.choice(self.n, self.batch_size, replace=False)\n",
    "        return (1 / self.batch_size) * sum(self.grad_f_i(self.w, i) for i in indices)\n",
    "    \n",
    "    def compute_L(self, num_samples=1000):\n",
    "        \"\"\"\n",
    "        Computes the Lipschitz constant L of the gradient of the objective function.\n",
    "\n",
    "        Args:\n",
    "            num_samples: The number of random samples to estimate L. Default is 1000.\n",
    "\n",
    "        Returns:\n",
    "            The estimated Lipschitz constant.\n",
    "        \"\"\"\n",
    "        L_vals = []\n",
    "        d = self.X_new.shape[1]\n",
    "        for _ in range(num_samples):\n",
    "            w1, w2 = np.random.randn(d), np.random.randn(d)\n",
    "            grad_diff = np.linalg.norm(self.grad_F(w1) - self.grad_F(w2), 2)\n",
    "            w_diff = np.linalg.norm(w1 - w2, 2)\n",
    "            \n",
    "            if w_diff > 1e-8: \n",
    "                L_vals.append(grad_diff / w_diff)\n",
    "        return max(L_vals) if L_vals else 1.0\n",
    "    \n",
    "    def compute_c(self):\n",
    "        \"\"\"\n",
    "        Computes the constant c associated with strong convexity (Assumption 4.5).\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): The number of random samples to estimate L. Default is 1000.\n",
    "\n",
    "        Returns:\n",
    "            float: The constant c.\n",
    "        \"\"\"\n",
    "        H = (1/self.m) * (self.X_new.T @ self.X_new)\n",
    "        eigenvalues = np.linalg.eigvalsh(H)\n",
    "        c = min(eigenvalues)\n",
    "        return c\n",
    "    \n",
    "    def estimate_parameters(self):\n",
    "        \"\"\"\n",
    "        Estimates the parameters bounding the variance of the gradient updates.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Estimated parameters (mu, mu_G, M, M_V, M_G).\n",
    "        \"\"\"\n",
    "        mu = 1 \n",
    "        mu_G = 1\n",
    "        M = 3\n",
    "        M_V = 1.5\n",
    "        M_G = M_V + mu_G ** 2\n",
    "        return mu, mu_G, M, M_V, M_G\n",
    "            # M equals noise times dimension\n",
    "            # and m_G is some dimension squared\n",
    "            # noise E [||x||^2] + E[(w-A)]2 E(||x||^4)\n",
    "        \n",
    "    def compute_fixed_stepsize(self):\n",
    "        \"\"\"\n",
    "        Computes the fixed stepsize for SGD using estimated parameters.\n",
    "\n",
    "        Returns:\n",
    "            float: The computed fixed stepsize for the SGD algorithm.\n",
    "        \"\"\"\n",
    "        L = self.compute_L()\n",
    "        c = self.compute_c()\n",
    "        M, mu, mu_G, M_V, M_G = self.estimate_parameters()\n",
    "        M_G = M_V + mu_G ** 2  \n",
    "        alpha = mu / (L * M_G)\n",
    "        print(f\"Parameters: L = {L}, c = {c}, M_V = {M_V}, mu = {mu}, mu_G = {mu_G}, M_G = {M_G} \\n Fixed Stepsize: alpha_bar = {alpha}\")\n",
    "        return alpha\n",
    "    \n",
    "#    def compute_diminishing_stepsize_params(self):\n",
    "    def optimize(self, stepsize_type='fixed'):\n",
    "        \"\"\"\n",
    "        Runs the SGD optimization process for a specified number of iterations.\n",
    "\n",
    "        Args:\n",
    "            stepsize_type: 'fixed' or 'diminishing' stepsize.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Optimized parameters, the history of the objective function, gradient norms, and \n",
    "                   distance to the optimal solution.\n",
    "        \"\"\"\n",
    "        alpha = self.compute_fixed_stepsize()\n",
    "    \n",
    "        w = self.w.copy()\n",
    "        obj_history = [self.F(w)]\n",
    "        grad_norm_history = [np.linalg.norm(self.grad_F(w))**2]\n",
    "        dist_to_opt_history = [np.linalg.norm(w - self.w_star)**2]\n",
    "    \n",
    "        for k in range(self.num_iterations):\n",
    "            alpha_k = alpha\n",
    "        \n",
    "            self.w = w\n",
    "            g_k = self.stochastic_grad()\n",
    "        \n",
    "            w -= alpha_k * g_k\n",
    "        \n",
    "            obj_history.append(self.F(w))\n",
    "            grad_norm_history.append(np.linalg.norm(self.grad_F(w))**2)\n",
    "            dist_to_opt_history.append(np.linalg.norm(w - self.w_star)**2)\n",
    "    \n",
    "        return w, np.array(obj_history), np.array(grad_norm_history), np.array(dist_to_opt_history)\n",
    "\n",
    "# mess around with step size\n",
    "# mess around with (x,y) from different distribution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': array([1., 2.]), 'b': 1.0}\n",
      "[0.99827838 1.00045838 2.00046671]\n",
      "Parameters: L = 8.937293745138895, c = 0.09179626698031591, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.01065625657541987\n",
      "True parameters:\n",
      "A: [1. 2.], b: 1.0\n",
      "Learned parameters (fixed step size): w_0 (bias) = 0.8959375945526598, w_1 = 1.0215856802565404, w_2 = 2.023343308508264\n"
     ]
    }
   ],
   "source": [
    "# generate random training data\n",
    "# generate training data\n",
    "# x_i has norm(2,0,1)\n",
    "# for i=1,...,n\n",
    "    # y = Ax_I + b + ita_i \n",
    "# generating (x_i,y_i) to be used for SGD\n",
    "# different ways of generating synthetic data to make it robust\n",
    "X, y, true_params = generate_training_data_fixed(m=200, n=2, noise=0.01)\n",
    "sgd = SGD(X, y, num_iterations=1000)\n",
    "w_fixed, obj_fixed, grad_fixed, dist_fixed = sgd.optimize(stepsize_type='fixed')\n",
    "print(\"True parameters:\")\n",
    "print(f\"A: {true_params['A']}, b: {true_params['b']}\")\n",
    "print(f\"Learned parameters (fixed step size): w_0 (bias) = {w_fixed[0]}, w_1 = {w_fixed[1]}, w_2 = {w_fixed[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments_with_fixed_parameters():\n",
    "    #stepsizes = [0.001, 0.01, 0.1]\n",
    "    num_steps = [100, 1000, 5000]\n",
    "    noise_levels = [0.01, 0.1, 1.0]\n",
    "    \n",
    "    \n",
    "    for j, n_steps in enumerate(num_steps):\n",
    "        for k, noise in enumerate(noise_levels):\n",
    "            X, y, true_params = generate_training_data_fixed(100, 2, noise, 'linear')\n",
    "            sgd = SGD(X, y, num_iterations=n_steps)\n",
    "            w, obj, grad, dist = sgd.optimize()\n",
    "            label = f\"Steps={n_steps}, Noise={noise}\"\n",
    "            print(\"Linear function:\")\n",
    "            print(f\"{label}, Final Loss: {obj[-1]:.4f}\")\n",
    "            \n",
    "    for j, n_steps in enumerate(num_steps):\n",
    "        for k, noise in enumerate(noise_levels):\n",
    "            X_poly, y_poly, true_params_poly = generate_training_data_fixed(100, 2, noise, 'polynomial')\n",
    "            sgd_poly = SGD(X_poly, y_poly, num_iterations=n_steps)\n",
    "            w_poly, obj_poly, grad_poly, dist_poly = sgd_poly.optimize()\n",
    "            label = f\"Steps={n_steps}, Noise={noise}\"\n",
    "            print(\"Polynomial function:\")\n",
    "            print(f\"{label}, Final Loss: {obj_poly[-1]:.4f}\")\n",
    "    \n",
    "    for j, n_steps in enumerate(num_steps):\n",
    "        for k, noise in enumerate(noise_levels):\n",
    "            X_nonlin, y_nonlin, true_params_nonlin = generate_training_data_fixed(\n",
    "            100, 1, noise, 'nonlinear', nonlinear_func=np.cos\n",
    "        )\n",
    "            sgd_nonlin = SGD(X_nonlin, y_nonlin, num_iterations=n_steps)\n",
    "            w_nonlin, obj_nonlin, grad_nonlin, dist_nonlin = sgd_nonlin.optimize()\n",
    "            label = f\"Steps={n_steps}, Noise={noise}\"\n",
    "            print(\"Nonlinear function\")\n",
    "            print(f\"{label}, Final Loss: {obj_nonlin[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': array([1., 2.]), 'b': 1.0}\n",
      "[1.00066158 0.99938748 2.00002144]\n",
      "Parameters: L = 9.714536339405711, c = 0.09879319836965189, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.009803668637459795\n",
      "Linear function:\n",
      "Steps=100, Noise=0.01, Final Loss: 0.0322\n",
      "{'A': array([1., 2.]), 'b': 1.0}\n",
      "[1.05420949 1.00927863 1.97258516]\n",
      "Parameters: L = 8.643081911187114, c = 0.10714207612316212, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.011018997183727307\n",
      "Linear function:\n",
      "Steps=100, Noise=0.1, Final Loss: 0.0504\n",
      "{'A': array([1., 2.]), 'b': 1.0}\n",
      "[0.37386605 1.14751455 2.13209764]\n",
      "Parameters: L = 9.74678876329626, c = 0.10870322757211717, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.009771227996315652\n",
      "Linear function:\n",
      "Steps=100, Noise=1.0, Final Loss: 0.6547\n",
      "{'A': array([1., 2.]), 'b': 1.0}\n",
      "[1.00142398 0.99934644 1.99970132]\n",
      "Parameters: L = 10.615121014317168, c = 0.09846818724218703, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.008971927414642061\n",
      "Linear function:\n",
      "Steps=1000, Noise=0.01, Final Loss: 0.0013\n",
      "{'A': array([1., 2.]), 'b': 1.0}\n",
      "[0.99822396 1.0116305  1.99580298]\n",
      "Parameters: L = 10.216459062657464, c = 0.09439543889232015, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.00932202582656092\n",
      "Linear function:\n",
      "Steps=1000, Noise=0.1, Final Loss: 0.0053\n",
      "{'A': array([1., 2.]), 'b': 1.0}\n",
      "[1.17407196 0.91929288 2.06301392]\n",
      "Parameters: L = 10.036228784469081, c = 0.07303775197417559, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.00948943047068385\n",
      "Linear function:\n",
      "Steps=1000, Noise=1.0, Final Loss: 0.6448\n",
      "{'A': array([1., 2.]), 'b': 1.0}\n",
      "[0.99671895 1.00064189 2.00080341]\n",
      "Parameters: L = 9.740135646519898, c = 0.09785346530264505, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.009777902351095422\n",
      "Linear function:\n",
      "Steps=5000, Noise=0.01, Final Loss: 0.0000\n",
      "{'A': array([1., 2.]), 'b': 1.0}\n",
      "[0.99196087 1.00443558 2.00885309]\n",
      "Parameters: L = 10.53540522574837, c = 0.10739746774098392, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.00903981320104658\n",
      "Linear function:\n",
      "Steps=5000, Noise=0.1, Final Loss: 0.0059\n",
      "{'A': array([1., 2.]), 'b': 1.0}\n",
      "[0.74260721 1.00643239 2.13630393]\n",
      "Parameters: L = 9.955830483875733, c = 0.09320524413846379, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.009566062358367891\n",
      "Linear function:\n",
      "Steps=5000, Noise=1.0, Final Loss: 0.4187\n",
      "[0.99794229 1.00274309 0.50009723 0.29905779 0.19998595]\n",
      "Parameters: L = 70.12944856642982, c = 0.023262438323417255, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0013580328547412059\n",
      "Polynomial function:\n",
      "Steps=100, Noise=0.01, Final Loss: 0.4195\n",
      "[1.1041542  0.97147507 0.43594604 0.30995726 0.21121995]\n",
      "Parameters: L = 71.82898910046345, c = 0.03310363747600626, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0013259005372453553\n",
      "Polynomial function:\n",
      "Steps=100, Noise=0.1, Final Loss: 0.3889\n",
      "[0.81466232 1.18671203 0.71907353 0.21944643 0.14916543]\n",
      "Parameters: L = 70.73076538564983, c = 0.06507021885698898, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0013464875534546041\n",
      "Polynomial function:\n",
      "Steps=100, Noise=1.0, Final Loss: 0.9894\n",
      "[1.00306655 0.99992967 0.49398037 0.30004005 0.20180734]\n",
      "Parameters: L = 60.19340452622771, c = 0.0425468638364727, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0015822015050934311\n",
      "Polynomial function:\n",
      "Steps=1000, Noise=0.01, Final Loss: 0.0782\n",
      "[1.0689572  0.95618537 0.46706724 0.31372878 0.20392112]\n",
      "Parameters: L = 72.96989058455254, c = 0.04235978656302631, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.001305169768998623\n",
      "Polynomial function:\n",
      "Steps=1000, Noise=0.1, Final Loss: 0.1048\n",
      "[0.94156352 1.01462017 0.40267062 0.32257176 0.2206064 ]\n",
      "Parameters: L = 70.05555367650136, c = 0.07180071551048049, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.001359465313455096\n",
      "Polynomial function:\n",
      "Steps=1000, Noise=1.0, Final Loss: 0.5787\n",
      "[1.00956959 0.99640879 0.49538931 0.30077117 0.20095443]\n",
      "Parameters: L = 59.773898248265326, c = 0.019242158182625552, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0015933057409528934\n",
      "Polynomial function:\n",
      "Steps=5000, Noise=0.01, Final Loss: 0.0014\n",
      "[0.98732286 0.98814826 0.49748989 0.30837414 0.20056298]\n",
      "Parameters: L = 72.83792586043238, c = 0.0537621525726774, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.001307534421292895\n",
      "Polynomial function:\n",
      "Steps=5000, Noise=0.1, Final Loss: 0.0081\n",
      "[0.50747064 0.80008655 0.90584977 0.3873087  0.1332349 ]\n",
      "Parameters: L = 65.82550080046217, c = 0.026300067002904652, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.0014468267476884364\n",
      "Polynomial function:\n",
      "Steps=5000, Noise=1.0, Final Loss: 0.4990\n",
      "[ 1.8947384  -0.56944122]\n",
      "Parameters: L = 6.15253266744255, c = 0.15177687340270818, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.015479494443331947\n",
      "Nonlinear function\n",
      "Steps=100, Noise=0.01, Final Loss: 0.2185\n",
      "[ 1.80550432 -0.5610749 ]\n",
      "Parameters: L = 6.1265512959888415, c = 0.1424216187388807, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.01554513961230509\n",
      "Nonlinear function\n",
      "Steps=100, Noise=0.1, Final Loss: 0.2347\n",
      "[ 1.52473429 -0.31012937]\n",
      "Parameters: L = 5.864442581542778, c = 0.19620916793794463, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.016239922876530345\n",
      "Nonlinear function\n",
      "Steps=100, Noise=1.0, Final Loss: 0.9380\n",
      "[ 1.87594297 -0.58146838]\n",
      "Parameters: L = 5.735598604812071, c = 0.1686180377652171, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.01660473505907336\n",
      "Nonlinear function\n",
      "Steps=1000, Noise=0.01, Final Loss: 0.0405\n",
      "[ 1.70584515 -0.47958748]\n",
      "Parameters: L = 6.014588393546605, c = 0.1793003952410358, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.01583451584821359\n",
      "Nonlinear function\n",
      "Steps=1000, Noise=0.1, Final Loss: 0.0565\n",
      "[ 2.0449831  -0.65360124]\n",
      "Parameters: L = 5.484620111978545, c = 0.20049262499645215, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.017364574627528512\n",
      "Nonlinear function\n",
      "Steps=1000, Noise=1.0, Final Loss: 0.7621\n",
      "[ 2.02365596 -0.64772486]\n",
      "Parameters: L = 5.684977592859727, c = 0.1643004206051626, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.016752589378314052\n",
      "Nonlinear function\n",
      "Steps=5000, Noise=0.01, Final Loss: 0.0206\n",
      "[ 1.99781234 -0.61200459]\n",
      "Parameters: L = 5.34070089981945, c = 0.18094577277239232, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.017832508695874525\n",
      "Nonlinear function\n",
      "Steps=5000, Noise=0.1, Final Loss: 0.0348\n",
      "[ 1.64052588 -0.39738787]\n",
      "Parameters: L = 6.294613081565314, c = 0.1953708733305256, M_V = 1.5, mu = 1, mu_G = 3, M_G = 10.5 \n",
      " Fixed Stepsize: alpha_bar = 0.015130095210619662\n",
      "Nonlinear function\n",
      "Steps=5000, Noise=1.0, Final Loss: 0.6354\n"
     ]
    }
   ],
   "source": [
    "run_experiments_with_fixed_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(15, 9))\n",
    "#plt.subplot(3, 1, 1)\n",
    "#plt.plot(obj_fixed - sgd.F_star, label='Fixed Stepsize')\n",
    "#plt.ylabel('$F(w) - F(w_*)$')\n",
    "#plt.title('Objective Function Decrease')\n",
    "#plt.legend()\n",
    "#plt.yscale('log')\n",
    "\n",
    "#plt.subplot(3, 1, 2)\n",
    "#plt.plot(grad_fixed, label='Fixed Stepsize')\n",
    "#plt.ylabel('$||F(w)||^2$')\n",
    "#plt.title('Gradient Norm Squared')\n",
    "#plt.legend()\n",
    "#plt.yscale('log')\n",
    "\n",
    "#plt.subplot(3, 1, 3)\n",
    "#plt.plot(dist_fixed, label='Fixed Stepsize')\n",
    "#plt.xlabel('Iteration')\n",
    "#plt.ylabel('$||w_k - w_*||^2$')\n",
    "#plt.title('Distance to Optimum')\n",
    "#plt.legend()\n",
    "#plt.yscale('log')\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
